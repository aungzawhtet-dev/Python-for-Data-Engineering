{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dfcfa8",
   "metadata": {},
   "source": [
    "✅ How it fits together\n",
    "\n",
    "Extract → Reads CSVs.\n",
    "\n",
    "Transform → Cleans and aggregates data.\n",
    "\n",
    "Load → Inserts into Postgres safely.\n",
    "\n",
    "Data Quality Check → Runs Soda checks after loading\n",
    "\n",
    "Logging → Tracks every step in etl_pipeline.log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5771bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2 # thie libary is use for connect to postgres\n",
    "from psycopg2.extras import execute_values # this library is use for insert data\n",
    "import logging\n",
    "import subprocess # this library is use for run command\n",
    "import os\n",
    "\n",
    "\n",
    "# define the file paths for the orders and customers data directory.\n",
    "base_dir = r\"D:\\DE\\ETL_Practices\\Practice_Coding\\8 Weeks Pyhton Practice for DE\\Week4_Data Pipelines in Pure Python\\Sample Dataset\"\n",
    "\n",
    "orders_path = os.path.join(base_dir, \"orders_data.csv\")\n",
    "customers_path = os.path.join(base_dir, \"customer_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be470309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Logging configuration\n",
    "# -----------------------\n",
    "logging.basicConfig(\n",
    "    filename='etl_pipeline_mini_project.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Extract function\n",
    "# -----------------------\n",
    "def extract(customers_path, orders_path):\n",
    "    try:\n",
    "        customers = pd.read_csv(customers_path)\n",
    "        orders = pd.read_csv(orders_path)\n",
    "        logging.info(\"Extract successful\")\n",
    "        return customers, orders \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extract failed: {e}\")\n",
    "        raise # raise the exception\n",
    "\n",
    "# -----------------------\n",
    "# Transform function\n",
    "# -----------------------\n",
    "def transform(customers, orders):\n",
    "    try:\n",
    "        # Clean customers\n",
    "        customers = customers.dropna(subset=['email']) # drop rows with missing email\n",
    "        customers = customers[customers['email'].str.contains('@')] # drop rows with invalid email,by creating a boolean mask (True/False for each row).\n",
    "        customers['signup_date'] = pd.to_datetime(customers['signup_date'])\n",
    "\n",
    "        # Clean orders\n",
    "        orders['order_date'] = pd.to_datetime(orders['order_date'])\n",
    "        orders = orders[orders['customer_id'].isin(customers['customer_id'])] # drop rows with invalid customer_id\n",
    "\n",
    "        # Aggregate: total amount per customer\n",
    "        orders_summary = orders.groupby('customer_id', as_index=False)['amount'].sum() # group by customer_id and sum the amount\n",
    "        orders_summary.rename(columns={'amount':'total_amount'}, inplace=True)\n",
    "\n",
    "        logging.info(\"Transform successful\")\n",
    "        return customers, orders, orders_summary\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transform failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# -----------------------\n",
    "# Data Quality Checks using Soda\n",
    "# -----------------------\n",
    "def run_data_quality_checks():\n",
    "    logging.info(\"Running Soda scan...\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\"soda\", \"scan\", \"-d\", \"postgres_db\", \"-c\", \"warehouse.yml\", \"checks.yml\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    # the above code will run the soda scan command and capture the output\n",
    "    logging.info(result.stdout)\n",
    "    logging.error(result.stderr)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        raise ValueError(\" Data quality checks failed\")\n",
    "\n",
    "    logging.info(\" Data quality checks passed\")\n",
    "    \n",
    "# -----------------------\n",
    "# Load function\n",
    "# -----------------------\n",
    "def load(customers, orders, orders_summary):\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            dbname=\"db_test\",\n",
    "            user=\"postgres\",\n",
    "            password=\"Aung24188\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS customers (\n",
    "                customer_id INT PRIMARY KEY,\n",
    "                first_name VARCHAR(50),\n",
    "                last_name VARCHAR(50),\n",
    "                email VARCHAR(100),\n",
    "                signup_date DATE\n",
    "            );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS orders (\n",
    "                order_id INT PRIMARY KEY,\n",
    "                customer_id INT REFERENCES customers(customer_id),\n",
    "                order_date DATE,\n",
    "                amount FLOAT,\n",
    "                status VARCHAR(20)\n",
    "            );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS orders_summary (\n",
    "                customer_id INT PRIMARY KEY,\n",
    "                total_amount FLOAT\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert data\n",
    "        execute_values(cur,\n",
    "                       \"INSERT INTO customers (customer_id, first_name, last_name, email, signup_date) VALUES %s ON CONFLICT (customer_id) DO NOTHING;\",\n",
    "                       customers.values.tolist())\n",
    "\n",
    "        execute_values(cur,\n",
    "                       \"INSERT INTO orders (order_id, customer_id, order_date, amount, status) VALUES %s ON CONFLICT (order_id) DO NOTHING;\",\n",
    "                       orders.values.tolist())\n",
    "\n",
    "        execute_values(cur,\n",
    "                       \"INSERT INTO orders_summary (customer_id, total_amount) VALUES %s ON CONFLICT (customer_id) DO NOTHING;\",\n",
    "                       orders_summary.values.tolist())\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        logging.info(\"Load successful\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Load failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# -----------------------\n",
    "# Main ETL Pipeline\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    customers_csv = customers_path\n",
    "    orders_csv = orders_path\n",
    "    logging.info(\"Logging ETL pipeline Mini Project started\")\n",
    "    \n",
    "    \n",
    "    # 1. Extract\n",
    "    customers_df, orders_df = extract(customers_csv, orders_csv)\n",
    "\n",
    "    # 2. Transform\n",
    "    customers_clean, orders_clean, orders_summary = transform(customers_df, orders_df)\n",
    "\n",
    "    # 3. Load\n",
    "    load(customers_clean, orders_clean, orders_summary)\n",
    "    \n",
    "    # 4. Data Quality Check (Soda after load into Postgres)\n",
    "    run_data_quality_checks()\n",
    "\n",
    "    logging.info(\"ETL pipeline with data quality checks completed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
