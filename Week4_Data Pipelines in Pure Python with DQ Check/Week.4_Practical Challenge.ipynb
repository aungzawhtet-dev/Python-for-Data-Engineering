{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4df02c5",
   "metadata": {},
   "source": [
    "ETL Pipeline for Two Datasets\n",
    "\n",
    "Here‚Äôs a medium difficulty version:\n",
    "\n",
    "Extract both CSVs.\n",
    "\n",
    "Transform:\n",
    "\n",
    "Sales ‚Üí clean + add total_amount.\n",
    "\n",
    "Customers ‚Üí clean + normalize location names.\n",
    "\n",
    "Load into the same SQLite DB but different tables (sales, customers).\n",
    "\n",
    "Logging + error handling throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6cc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "\n",
    "# define the file paths for the sales and customers data directory.\n",
    "base_dir = r\"D:\\DE\\ETL_Practices\\Practice_Coding\\8 Weeks Pyhton Practice for DE\\Week4_Data Pipelines in Pure Python\\Sample Dataset\"\n",
    "\n",
    "sales_path = os.path.join(base_dir, \"sales_data.csv\")\n",
    "customers_path = os.path.join(base_dir, \"customers_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_df is \n",
      "    order_id  order_date customer_name     product  quantity  price\n",
      "0      1001  2025-01-01      John Doe      Laptop         1   1200\n",
      "1      1002  2025-01-02    Jane Smith       Phone         2    600\n",
      "2      1003  2025-01-03       Bob Lee  Headphones         3    100\n",
      "3      1004  2025-01-03      Anna Kim      Laptop         1   1200\n",
      "4      1005  2025-01-04      John Doe       Phone         1    600\n",
      "customers_df is \n",
      "   customer_id customer_name             email   location\n",
      "0        C001      John Doe  john@example.com        USA\n",
      "1        C002    Jane Smith  jane@example.com     Canada\n",
      "2        C003       Bob Lee   bob@example.com         UK\n",
      "3        C004      Anna Kim  anna@example.com      Korea\n",
      "4        C005     Mike Chan  mike@example.com  Singapore\n",
      "cleasn_sales is \n",
      "    order_id  order_date customer_name     product  quantity  price  \\\n",
      "0      1001  2025-01-01      John Doe      Laptop         1   1200   \n",
      "1      1002  2025-01-02    Jane Smith       Phone         2    600   \n",
      "2      1003  2025-01-03       Bob Lee  Headphones         3    100   \n",
      "3      1004  2025-01-03      Anna Kim      Laptop         1   1200   \n",
      "4      1005  2025-01-04      John Doe       Phone         1    600   \n",
      "\n",
      "   total_amount  \n",
      "0          1200  \n",
      "1          1200  \n",
      "2           300  \n",
      "3          1200  \n",
      "4           600  \n",
      "clean_customers is \n",
      "   customer_id customer_name             email   location\n",
      "0        C001      John Doe  john@example.com        Usa\n",
      "1        C002    Jane Smith  jane@example.com     Canada\n",
      "2        C003       Bob Lee   bob@example.com         Uk\n",
      "3        C004      Anna Kim  anna@example.com      Korea\n",
      "4        C005     Mike Chan  mike@example.com  Singapore\n",
      "fact_sales_table is \n",
      "    order_id  order_date customer_name     product  quantity  price  \\\n",
      "0      1001  2025-01-01      John Doe      Laptop         1   1200   \n",
      "1      1002  2025-01-02    Jane Smith       Phone         2    600   \n",
      "2      1003  2025-01-03       Bob Lee  Headphones         3    100   \n",
      "3      1004  2025-01-03      Anna Kim      Laptop         1   1200   \n",
      "4      1005  2025-01-04      John Doe       Phone         1    600   \n",
      "\n",
      "   total_amount customer_id             email location  \n",
      "0          1200        C001  john@example.com      Usa  \n",
      "1          1200        C002  jane@example.com   Canada  \n",
      "2           300        C003   bob@example.com       Uk  \n",
      "3          1200        C004  anna@example.com    Korea  \n",
      "4           600        C001  john@example.com      Usa  \n"
     ]
    }
   ],
   "source": [
    "# Configure Logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"etl_pipeline_day6_extended.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "# Extract Function\n",
    "\n",
    "def extract(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract data from CSV into a DataFrame\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\" Extracted {file_path} successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Extraction failed for {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Transform Functions\n",
    "\n",
    "def transform_sales(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform sales dataset\"\"\"\n",
    "    try:\n",
    "        if df.empty:\n",
    "            logging.warning(\" Sales data empty\")\n",
    "            return df\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.fillna({\"customer_name\": \"Unknown\"})\n",
    "        df[\"total_amount\"] = df[\"quantity\"] * df[\"price\"]\n",
    "\n",
    "        logging.info(\" Sales transformation successful\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Sales transformation failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def transform_customers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform customers dataset\"\"\"\n",
    "    try:\n",
    "        if df.empty:\n",
    "            logging.warning(\" Customers data empty\")\n",
    "            return df\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.fillna({\"location\": \"Unknown\"})\n",
    "        df[\"location\"] = df[\"location\"].str.title()\n",
    "\n",
    "        logging.info(\" Customers transformation successful\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Customers transformation failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def join_sales_customers(sales_df: pd.DataFrame, customers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join sales and customers to build fact table\"\"\"\n",
    "    try:\n",
    "        if sales_df.empty or customers_df.empty:\n",
    "            logging.warning(\" One or both datasets are empty, cannot join\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        fact_df = sales_df.merge(\n",
    "            customers_df,\n",
    "            on=\"customer_name\",\n",
    "            how=\"left\"  # keep all sales even if customer missing\n",
    "        )\n",
    "\n",
    "        logging.info(\" Joined sales and customers successfully\")\n",
    "        return fact_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Join failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Load Function\n",
    "\n",
    "def load(df: pd.DataFrame, db_name: str, table_name: str):\n",
    "    \"\"\"Load DataFrame into SQLite DB\"\"\"\n",
    "    try:\n",
    "        if df.empty:\n",
    "            logging.warning(f\" No data to load into {table_name}\")\n",
    "            return\n",
    "\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "        conn.close()\n",
    "\n",
    "        logging.info(f\" Loaded data into {table_name} table successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Load failed for {table_name}: {e}\")\n",
    "\n",
    "\n",
    "# Main ETL Pipeline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\" Day 6 Extended ETL pipeline started\")\n",
    "\n",
    "    # Extract\n",
    "    sales_df = extract(sales_path)\n",
    "    customers_df = extract(customers_path)\n",
    "    print(f\"sales_df is \\n {sales_df.head()}\")\n",
    "    print(f\"customers_df is \\n {customers_df.head()}\")\n",
    "\n",
    "    # Transform\n",
    "    clean_sales = transform_sales(sales_df)\n",
    "    clean_customers = transform_customers(customers_df)\n",
    "    print(f\"clean_sales is \\n {clean_sales.head()}\")\n",
    "    print(f\"clean_customers is \\n {clean_customers.head()}\")\n",
    "\n",
    "    # Join\n",
    "    fact_sales_table = join_sales_customers(clean_sales, clean_customers)\n",
    "    print(f\"fact_sales_table is \\n {fact_sales_table.head()}\")\n",
    "\n",
    "    # Load into DB (3 tables)\n",
    "    load(clean_sales, \"day6_extended_etl.db\", \"sales\")\n",
    "    load(clean_customers, \"day6_extended_etl.db\", \"customers\")\n",
    "    load(fact_sales_table, \"day6_extended_etl.db\", \"fact_sales\")\n",
    "    \n",
    "\n",
    "    logging.info(\"üèÅ Day 6 Extended ETL pipeline finished\")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
